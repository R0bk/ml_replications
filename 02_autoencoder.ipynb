{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPi3hnvl1SBzQRRTPZuZx8u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/R0bk/ml_replications/blob/main/02_autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLyt1RtWEeaU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58db3f76-64ac-4d37-92c1-c3437174a3b4"
      },
      "source": [
        "!pip install torch torchvision\n",
        "!pip install numpy tqdm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.9.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.19.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deDJDu0jEdnW"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda, Compose\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_tKp9TGGKuo",
        "outputId": "7960b244-4d23-42e8-b829-2b9f2ec56afa"
      },
      "source": [
        "# Download training data from open datasets.\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "# Download test data from open datasets.\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
            "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tXX_TBcGSRH",
        "outputId": "3b8030ff-cfb8-4613-adac-5af5b3d95128"
      },
      "source": [
        "batch_size = 128\n",
        "\n",
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "for X, y in test_dataloader:\n",
        "    print(\"Shape of X [N, C, H, W]: \", X.shape)\n",
        "    print(\"Shape of y: \", y.shape, y.dtype)\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X [N, C, H, W]:  torch.Size([128, 1, 28, 28])\n",
            "Shape of y:  torch.Size([128]) torch.int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qzLAYPHGYUH",
        "outputId": "c01f2698-7f87-4fc3-f5a6-cad68e9f0b0d"
      },
      "source": [
        "# Get cpu or gpu device for training.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using {} device\".format(device))\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(NeuralNetwork, self).__init__()\n",
        "    self.flatten = nn.Flatten()\n",
        "\n",
        "    self.encoder = nn.Sequential(\n",
        "        nn.Linear(28*28, 128),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(128, 64),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(64, 10),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "    self.decoder = nn.Sequential(\n",
        "        nn.Linear(10, 64),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(64, 128),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(128, 28*28),\n",
        "    )\n",
        "    # self.unflatten = nn.Unflatten(1, torch.Size([28, 28]))\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.flatten(x)\n",
        "    x = self.encoder(x)\n",
        "    x = self.decoder(x)\n",
        "    # x = self.unflatten(x)\n",
        "    return x\n",
        "\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (encoder): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=128, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=64, out_features=10, bias=True)\n",
            "    (5): ReLU()\n",
            "  )\n",
            "  (decoder): Sequential(\n",
            "    (0): Linear(in_features=10, out_features=64, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=64, out_features=128, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=128, out_features=784, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSrh075GHWZQ"
      },
      "source": [
        "loss_fn = nn.MSELoss()\n",
        "optimiser = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OLZjO6xH1iX"
      },
      "source": [
        "def train(dataloader, model, loss_fn, optimiser):\n",
        "  size = len(train_dataloader.dataset)\n",
        "\n",
        "  model.train()\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    X, y = X.to(device), y.to(device)\n",
        "    # print('hi')\n",
        "    pred = model(X)\n",
        "    # print(pred.shape)\n",
        "    # print(X.squeeze().flatten(1).shape)\n",
        "    loss = loss_fn(pred, X.squeeze().flatten(1))\n",
        "    \n",
        "\n",
        "    optimiser.zero_grad()\n",
        "    loss.backward()\n",
        "    optimiser.step()\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      loss, current = loss.item(), batch * len(X)\n",
        "      print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpM50IVDIl0A"
      },
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_d-pp-4IzHv",
        "outputId": "c856817d-f701-4725-bc76-97162f1274cf"
      },
      "source": [
        "epochs = 20\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimiser)\n",
        "    # test(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 0.018287  [    0/60000]\n",
            "loss: 0.016943  [12800/60000]\n",
            "loss: 0.017641  [25600/60000]\n",
            "loss: 0.017229  [38400/60000]\n",
            "loss: 0.017243  [51200/60000]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.018304  [    0/60000]\n",
            "loss: 0.016857  [12800/60000]\n",
            "loss: 0.017627  [25600/60000]\n",
            "loss: 0.017192  [38400/60000]\n",
            "loss: 0.017225  [51200/60000]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.018290  [    0/60000]\n",
            "loss: 0.016791  [12800/60000]\n",
            "loss: 0.017538  [25600/60000]\n",
            "loss: 0.017143  [38400/60000]\n",
            "loss: 0.017183  [51200/60000]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.018275  [    0/60000]\n",
            "loss: 0.016762  [12800/60000]\n",
            "loss: 0.017503  [25600/60000]\n",
            "loss: 0.017098  [38400/60000]\n",
            "loss: 0.017096  [51200/60000]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.018301  [    0/60000]\n",
            "loss: 0.016717  [12800/60000]\n",
            "loss: 0.017497  [25600/60000]\n",
            "loss: 0.017063  [38400/60000]\n",
            "loss: 0.017072  [51200/60000]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.018285  [    0/60000]\n",
            "loss: 0.016682  [12800/60000]\n",
            "loss: 0.017513  [25600/60000]\n",
            "loss: 0.017051  [38400/60000]\n",
            "loss: 0.017031  [51200/60000]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.018260  [    0/60000]\n",
            "loss: 0.016651  [12800/60000]\n",
            "loss: 0.017463  [25600/60000]\n",
            "loss: 0.017046  [38400/60000]\n",
            "loss: 0.016999  [51200/60000]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.018222  [    0/60000]\n",
            "loss: 0.016607  [12800/60000]\n",
            "loss: 0.017443  [25600/60000]\n",
            "loss: 0.017057  [38400/60000]\n",
            "loss: 0.016947  [51200/60000]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.018231  [    0/60000]\n",
            "loss: 0.016548  [12800/60000]\n",
            "loss: 0.017431  [25600/60000]\n",
            "loss: 0.016979  [38400/60000]\n",
            "loss: 0.016910  [51200/60000]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.018157  [    0/60000]\n",
            "loss: 0.016522  [12800/60000]\n",
            "loss: 0.017449  [25600/60000]\n",
            "loss: 0.016943  [38400/60000]\n",
            "loss: 0.016889  [51200/60000]\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 0.018142  [    0/60000]\n",
            "loss: 0.016519  [12800/60000]\n",
            "loss: 0.017401  [25600/60000]\n",
            "loss: 0.016907  [38400/60000]\n",
            "loss: 0.016867  [51200/60000]\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 0.018110  [    0/60000]\n",
            "loss: 0.016489  [12800/60000]\n",
            "loss: 0.017376  [25600/60000]\n",
            "loss: 0.016880  [38400/60000]\n",
            "loss: 0.016825  [51200/60000]\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 0.018047  [    0/60000]\n",
            "loss: 0.016459  [12800/60000]\n",
            "loss: 0.017354  [25600/60000]\n",
            "loss: 0.016855  [38400/60000]\n",
            "loss: 0.016815  [51200/60000]\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.018054  [    0/60000]\n",
            "loss: 0.016416  [12800/60000]\n",
            "loss: 0.017332  [25600/60000]\n",
            "loss: 0.016827  [38400/60000]\n",
            "loss: 0.016797  [51200/60000]\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.018048  [    0/60000]\n",
            "loss: 0.016360  [12800/60000]\n",
            "loss: 0.017310  [25600/60000]\n",
            "loss: 0.016795  [38400/60000]\n",
            "loss: 0.016784  [51200/60000]\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.018025  [    0/60000]\n",
            "loss: 0.016337  [12800/60000]\n",
            "loss: 0.017213  [25600/60000]\n",
            "loss: 0.016747  [38400/60000]\n",
            "loss: 0.016760  [51200/60000]\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.018014  [    0/60000]\n",
            "loss: 0.016298  [12800/60000]\n",
            "loss: 0.017241  [25600/60000]\n",
            "loss: 0.016754  [38400/60000]\n",
            "loss: 0.016747  [51200/60000]\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.017956  [    0/60000]\n",
            "loss: 0.016252  [12800/60000]\n",
            "loss: 0.017215  [25600/60000]\n",
            "loss: 0.016724  [38400/60000]\n",
            "loss: 0.016734  [51200/60000]\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.017948  [    0/60000]\n",
            "loss: 0.016241  [12800/60000]\n",
            "loss: 0.017189  [25600/60000]\n",
            "loss: 0.016704  [38400/60000]\n",
            "loss: 0.016738  [51200/60000]\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.017908  [    0/60000]\n",
            "loss: 0.016207  [12800/60000]\n",
            "loss: 0.017161  [25600/60000]\n",
            "loss: 0.016668  [38400/60000]\n",
            "loss: 0.016734  [51200/60000]\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxqyxNgVI3yP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "293a11af-692f-40f8-c06e-b65d59a6f9fb"
      },
      "source": [
        "def plot(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            print(X)\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            print(pred)\n",
        "            plt.figure()\n",
        "            plt.imshow(  X.cpu()[0].squeeze()  )\n",
        "            plt.imshow(  pred.cpu().unflatten(1, torch.Size([28, 28]))[0]  )\n",
        "            break\n",
        "plot(test_dataloader, model, loss_fn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          ...,\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
            "\n",
            "\n",
            "        [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          ...,\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
            "\n",
            "\n",
            "        [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          ...,\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          ...,\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
            "\n",
            "\n",
            "        [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          ...,\n",
            "          [0.0078, 0.0000, 0.0000,  ..., 0.7216, 0.5922, 0.0000],\n",
            "          [0.0039, 0.0000, 0.0000,  ..., 0.0431, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0039, 0.0000,  ..., 0.0000, 0.0431, 0.0000]]],\n",
            "\n",
            "\n",
            "        [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          ...,\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]]])\n",
            "tensor([[ 3.4536e-05, -5.6478e-04, -6.0970e-04,  ..., -9.0478e-03,\n",
            "          1.4795e-03, -5.5808e-04],\n",
            "        [-7.1118e-04, -9.2831e-04, -8.6860e-04,  ...,  2.4340e-02,\n",
            "         -9.7003e-03,  3.3457e-04],\n",
            "        [-1.6197e-04, -7.3128e-04, -4.8704e-04,  ...,  8.5507e-03,\n",
            "          5.3656e-04, -4.5257e-04],\n",
            "        ...,\n",
            "        [-4.4205e-04, -7.2051e-04,  2.0479e-04,  ...,  6.6493e-03,\n",
            "          6.0730e-03, -1.0780e-03],\n",
            "        [-2.2698e-04, -5.9096e-04, -1.0468e-03,  ...,  7.6876e-02,\n",
            "          3.5984e-02,  1.8164e-03],\n",
            "        [-4.3102e-04, -9.2094e-04, -1.0985e-03,  ...,  1.9644e-02,\n",
            "          9.1759e-03, -4.1101e-03]], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAWVElEQVR4nO3dTYxkV3UH8P959dXVPT09Hs+HG8/YGMeOMBCGqOOEDyVEKMh4Y7NBeEEcCWVYgAQSi1jOAi+tKIBYREhDsDARASEBwgsrwVhIFhJYjImxPTiJDbFhhvF82Z7p766qd7LoMmqbuf/TrlddVfL9/6RWd9ft996tV+9UVde5515zd4jIG18x7g6IyGgo2EUyoWAXyYSCXSQTCnaRTNRHebBmre3txtwoDzk8LGlhYzx2pGrfrOIOWLYnul+14LWoLF93d7av4v2ONt+hLNhq5yI2equXPXqlYDezWwB8CUANwL+6+73s79uNObz7mr+tcsixsV76wvLgorSy4gNb5cIIgtWL4KpsBJdIdN96vWSTddNtAFDumqbttrbOj13liarqk1y0fZUnKnI9/OQ3X0+2Dfw23sxqAP4FwIcA3ATgDjO7adD9icjOqvI/+80AnnX3X7v7BoBvAbhtON0SkWGrEuxXA/jtlt9P9m97FTM7ambHzez4Rm+1wuFEpIod/zTe3Y+5+4K7LzRr7Z0+nIgkVAn2UwAOb/n9UP82EZlAVYL9ZwBuMLPrzKwJ4KMAHhhOt0Rk2AZOvbl718w+BeA/sZl6u8/dTwytZ69XETxvRamQIH3mne7gx+4EKaJaLdieHBug6S2fCf51ajaqHTu477beSTeGKctqeXSfbpFGnjK0XpBSDNKG4RgBOv5gZ3LwlfLs7v4ggAeH1BcR2UEaLiuSCQW7SCYU7CKZULCLZELBLpIJBbtIJkZaz14ZyUd7UIppJBcNACAlrJs7SOfpbTkY8x/lTaM8ezRGYCqdT3bSth0WndeVtcF3Ho1PCNjaBm8nufByNy+f9Ro/54bgMdvBsuRB6ZVdJBMKdpFMKNhFMqFgF8mEgl0kEwp2kUyMPvXG0gpRuSUreYwqCoNUiZVBKSdTsUw0nJ12g6dxvM5SksH9DlJE7tVmn7WCPGYbpPwVAMj9AgCfnuLbs3TpWnDscBrr4LxF56VBzus6TykOmprTK7tIJhTsIplQsItkQsEukgkFu0gmFOwimVCwi2RitHl2s0pljU5KQcOpf6N8crtJ2201yH0yQb646vas7+U0HwNQBPlmCyqDw5wvOe++FpTHdoJceMDq5PKOxkZ0g/sV5MItuM69RY4fnVPanm7TK7tIJhTsIplQsItkQsEukgkFu0gmFOwimVCwi2Ri9PXsBckRBvXNRnKjXg9qwrvBVNHR1L8sXxzkwS2q244EeVdvkocxqLu2Dk+kF0s8F+4XL/F2UsvvXV7nX66s0PZIMTubbts1Q7f1XXyqaau4BDidFn2KjwGwFbYEePrxrhTsZvYcgEUAPQBdd1+osj8R2TnDeGX/a3c/P4T9iMgO0v/sIpmoGuwO4Adm9piZHb3cH5jZUTM7bmbHN3rV/gcTkcFVfRv/Pnc/ZWYHADxkZv/t7o9s/QN3PwbgGADMtecrLIAlIlVUemV391P972cBfA/AzcPolIgM38DBbmYzZjb7ys8APgjgqWF1TESGq8rb+IMAvtfPN9YB/Lu7/0e4Fcv7RjXnwTK6FFm+F9hGLpzNQR5tGx2bbx3m8ctmur3YCI4d5NmjsQ9lkGdn+ezi4H66rS0u0fbe+Qu0vVxOf0ZkrWD+gmA5aA9q7dmYEAAoZ4M57wfdN7lOBw52d/81gHcOur2IjJZSbyKZULCLZELBLpIJBbtIJhTsIpkYbYmrIy4lJeh00dG0w0FJYpRqof3uRfMtB4LlfaP2YjVdKlosrdJtrReU/gZlqDD+etF5x1uSbYvXtOi2RXBadz87T9ttI933kpUFA+F1GpVMd6d5am9jT7q9eSlI662Rx4Rc53plF8mEgl0kEwp2kUwo2EUyoWAXyYSCXSQTCnaRTIx4yWbwfHc0PS8pFbVgCV2PluiN8vQNMo11UMJKp88GwmmHbZ33rWDlt1HfImzZYwDFdYdp++/+rJ1sWz4c5PiD0/bSjbtpe0FOWy0YVlEE7bWNaOpx3lw20ndu/3/xgxcrZHrvMn1O9coukgkFu0gmFOwimVCwi2RCwS6SCQW7SCYU7CKZGHE9u9M8YIRO2Rzkk32OL9EbTh08la4/tqjmO5i22Av+nOvBU3I5k64LL6J69ejxCMYAvPwuPh304lvTj9nu/Xyq6GadP6brXT7F9tpq+ryvdfn9Kpf4uIxiPRgbEQ1vIKf9wGPBlOp0anHVs4tkT8EukgkFu0gmFOwimVCwi2RCwS6SCQW7SCZGPm88q6/2aP70CnPOe43nZMP50RvT6baoVj4S3e8gF87mR4+Wkw6Xg97Dxye8/Ef89aI1t5xsazd539669wxtP7e2i7Yvz6Tz7L2S9/t8m9/vbpCn7yzzsRWsVn9tP1/OeeYCG5+QjpHwld3M7jOzs2b21Jbb9prZQ2b2TP/7FdF+RGS8tvM2/msAbnnNbXcBeNjdbwDwcP93EZlgYbC7+yMAXnzNzbcBuL//8/0Abh9yv0RkyAb9n/2gu5/u//wCgIOpPzSzowCOAsBUfXbAw4lIVZU/jXd3B/lUwN2PufuCuy80C/Ihl4jsqEGD/YyZzQNA//vZ4XVJRHbCoMH+AIA7+z/fCeD7w+mOiOyU8H92M/smgPcD2GdmJwF8DsC9AL5tZh8H8DyAjwylN1EendVmR7noYN8e1XWzOe2DevSI14K144OuFRfTuexINK/86lX8X6/uLn5e2Zkx49uu9vj4hd0NMn86gD3N9Nr0jWDx9yLo2/klnof3ko/bmGqn50+4dO0eum37dPoxcTL/QBjs7n5HoukD0bYiMjk0XFYkEwp2kUwo2EUyoWAXyYSCXSQTI1+ymaUGQqwUNFgWOUpvsWWPAaBk20/zksRomusodWerK7Tdl9KpN5vhqbPOPC9YPHeEXyLd3UEKq5Pevtvj5bVFtO5xYKmTnmJ7us6nDm/VeOpsfu4Sbe/O8sf08K6Xkm0/PTRHty0b5LyR61iv7CKZULCLZELBLpIJBbtIJhTsIplQsItkQsEukonR5tmBSqWizspUi+CuBHl0TKVzsgDgJLdZkDw3gMrLImNtnbcfuDLZtHINz9leeBsvI13fy/vuDd7eaKbz1d1eMA11kOs+0Fqk7adW06Wi+1p8uegDU3zfNbbmMoClHr+ermql8/SPXsevp3JqsPEmemUXyYSCXSQTCnaRTCjYRTKhYBfJhIJdJBMKdpFMjD7PzgT5ZuuQvGswVXSxHOSqg5rz4lJ6WmKs8imNfaZN28tWsOTz4QO0+cLb00sXL13Dxxd0ZoM8ep2fV2sGU3iTKZnn2vy8zTXIOQfQDZZdZtNFH5n5Dd12tuB9O9flS5md3NhL26eLdD39e679P7rt840/TrY5G8ZC9yoibxgKdpFMKNhFMqFgF8mEgl0kEwp2kUwo2EUyMfo8O8uHRzXnLM9e53OQR/v2Nq8/Lnenc+U2zbftzTRp+6W38Dz88lX8OXl1Pp3rLoM8ecFLxuFNvn2tyccn7JtN12ZfNcPnXr+m9SJt74E/prvr6Vz53hqvZ19zPvbhxW56bAMQ17Mz7577FW1/duamZJtXqWc3s/vM7KyZPbXltnvM7JSZPd7/ujXaj4iM13bexn8NwC2Xuf2L7n6k//XgcLslIsMWBru7PwKAv58SkYlX5QO6T5nZE/23+ckFw8zsqJkdN7PjGz0+1llEds6gwf5lANcDOALgNIDPp/7Q3Y+5+4K7LzRr/IMoEdk5AwW7u59x9567lwC+AuDm4XZLRIZtoGA3s/ktv34YwFOpvxWRyRDm2c3smwDeD2CfmZ0E8DkA7zezIwAcwHMAPrGto5nxNdYjZNtyF18jnc37DgDdgzxvujyfzpXTtdsBrFzF25ev5blqn+ZridNlzNf583lvKsij7+KJ+Hab923PVPpzmrrxWviO88dssccf89Nr6TnzDzYu0m2jPPpKycdONIw/puvl4HHQmU5fT04e7vCI7n7HZW7+6nY6JSKTQ8NlRTKhYBfJhIJdJBMKdpFMKNhFMjHSElcvDOVMuvRv5dA03b7bSj83re8Jpkye4e3re3kKauMAm8aabgqb4mmYZrtD28uS951V7/bawXTLDd632Wk+pfI1u1+i7W9qp1Nci12eOrvY5SMuo9Rch+Shpgs+tXinxkOjDMpro9TbmY3dybb1kpfX9lqDzRetV3aRTCjYRTKhYBfJhIJdJBMKdpFMKNhFMqFgF8nEaPPs9QLrV6Zzqy/8Oc+blo10QjtaWtjrwdLCc7xUc5qUmXY6wTTWgdlpnvNt1XmZKWuPykhrBW9/29xp2l6QJZkBoCRrCC9HAxQCbN8A0CJLNkf21/k01w3jj8nFHh8zsreRnmJ7rpZuAwA2vICdUb2yi2RCwS6SCQW7SCYU7CKZULCLZELBLpIJBbtIJkaaZy+bhqWr07W6e/7kPN1+tpXOR7dqPO95ZolPDbx7iue6neWLN/i0wt0ef07d216h7es9/jCx+75vii9NvLvO7/cVdd63iz1ec87y8PUgDz5X58uFRfXuBRljsKfG79fV9Zdpe1RLvwHevkZq1g/X+TTXbG4GNpW0XtlFMqFgF8mEgl0kEwp2kUwo2EUyoWAXyYSCXSQTI82zFx3HzAvpnPCpE/vo9svXp2uM33PoObrtO/b8jra/tc3bDzcupPtVpufCB4Bz3fQc4dG+AWCx5Plk5s0NPnYhyhdPBXXbl5zf9xVybi6VfN74yGKdn5cb2+k8+1+1+TmfK/i+V0o+/8GZHh8jcM7S5+X57hV024Icmk0vEL6ym9lhM/uRmf3SzE6Y2af7t+81s4fM7Jn+d95DERmr7byN7wL4rLvfBOAvAHzSzG4CcBeAh939BgAP938XkQkVBru7n3b3n/d/XgTwNICrAdwG4P7+n90P4Pad6qSIVPe6PqAzszcDeBeARwEcdPdXJih7AcDBxDZHzey4mR3vbPC5tURk52w72M1sF4DvAPiMu7/qkzJ3dyTmunP3Y+6+4O4LjeZMpc6KyOC2Fexm1sBmoH/D3b/bv/mMmc332+cBnN2ZLorIMISpNzMzAF8F8LS7f2FL0wMA7gRwb//798N9dR3Nl9J5g6t+wp97Or+YTbb99E3vpNuu7QumPH4TX5p4fn+67PDQLC+HbBY8fXWwtUjbw3LKMv0wFsF0zc9c2k/bz6/wKZFfPJ9+TADALqVLOVsXguWkg//6ggpXrN2Yfky/ey1PtV7Z4gdf7PCU49kVfl6WN9Ln5eIiP+fzL6RLgwuy+vd28uzvBfAxAE+a2eP92+7GZpB/28w+DuB5AB/Zxr5EZEzCYHf3HwPJlec/MNzuiMhO0XBZkUwo2EUyoWAXyYSCXSQTCnaRTIy0xNV6JeoX06V/USHndC9dsrjrd+m8JQDUlkkCEkBvmm/fmU2X3z4/e4BuG6yajBNzfOnhCMutWrBq8cwZPgbgijW+g/1LfCpqIN1eu8DHF8D4efEGv3zXD80l207eeB3d9iRtBWp8WAaCoRWokZfZ+UV+wez6VbrUu7ZOcvC8SyLyRqFgF8mEgl0kEwp2kUwo2EUyoWAXyYSCXSQTI82zAwCcLOFLcvAAgC7L+fIa4GKF59mtw3Ob9cV0vni6y7e1Ds9Vl22e4y9b/GEycvyyyWvha6vB+IOobw2+/2ItnXD2Kb7UdZRnt2V+vUydSGfL963N82OT6xQAinWeSGePSbR/i/bdIe1l+rh6ZRfJhIJdJBMKdpFMKNhFMqFgF8mEgl0kEwp2kUyMNs/uDrAcYY3nbFkO0UitOwDYRpRnDwqQ6+m+eS2oR6fjAwBv8KWLyxY/L0aKo8N8b9DMlgAGgGKDnzdvpPvmveDxjvLs63zZZF9J5+FrQR1+eN6C62lHsTEAVZZsFpE3BgW7SCYU7CKZULCLZELBLpIJBbtIJhTsIpnYzvrshwF8HcBBbGbxjrn7l8zsHgB/D+Bc/0/vdvcH6c48yIev8bwpmry2upKgftlWSV62CPLsF5doc53tG0Btmufho75X2daifZOxDwBgZB5zWwvmnI/mje/wXLezmvFVfq1F4zLoeBEAKCq8jgbXE50vn2y6nUE1XQCfdfefm9ksgMfM7KF+2xfd/Z+3sQ8RGbPtrM9+GsDp/s+LZvY0gKt3umMiMlyv672Gmb0ZwLsAPNq/6VNm9oSZ3WdmVyS2OWpmx83s+Ea5UqmzIjK4bQe7me0C8B0An3H3SwC+DOB6AEew+cr/+ctt5+7H3H3B3ReaBZ8nTkR2zraC3cwa2Az0b7j7dwHA3c+4e8/dSwBfAXDzznVTRKoKg93MDMBXATzt7l/YcvvW6Tk/DOCp4XdPRIZlO5/GvxfAxwA8aWaP92+7G8AdZnYEm+m45wB8YltHZKmcXrC+8CpZjrbF03K2EqyxS0pYAcBZe1SK2QqmTA6OHfaNlNhG01hH5bdhai7Yv7Oy5eC8he1l0LcpkrKMznl0XqK+RSlLtn1U6h0dO2E7n8b/GJfP3vGcuohMFI2gE8mEgl0kEwp2kUwo2EUyoWAXyYSCXSQTo51K2ni+2oqglJMJcq5gZYFAPDUwm0q6HeTRI1FONsj52gaZYjuaxjrIN1swVXRU6lmsk/MalokG+eTpNm32dotvz0RjPqJcN5neO+LR/R6wpFmv7CKZULCLZELBLpIJBbtIJhTsIplQsItkQsEukglj0+0O/WBm5wA8v+WmfQDOj6wDr8+k9m1S+wWob4MaZt+udff9l2sYabD/wcHNjrv7wtg6QExq3ya1X4D6NqhR9U1v40UyoWAXycS4g/3YmI/PTGrfJrVfgPo2qJH0baz/s4vI6Iz7lV1ERkTBLpKJsQS7md1iZv9jZs+a2V3j6EOKmT1nZk+a2eNmdnzMfbnPzM6a2VNbbttrZg+Z2TP975ddY29MfbvHzE71z93jZnbrmPp22Mx+ZGa/NLMTZvbp/u1jPXekXyM5byP/n93MagD+F8DfADgJ4GcA7nD3X460Iwlm9hyABXcf+wAMM/tLAEsAvu7ub+/f9k8AXnT3e/tPlFe4+z9MSN/uAbA07mW8+6sVzW9dZhzA7QD+DmM8d6RfH8EIzts4XtlvBvCsu//a3TcAfAvAbWPox8Rz90cAvPiam28DcH//5/uxebGMXKJvE8HdT7v7z/s/LwJ4ZZnxsZ470q+RGEewXw3gt1t+P4nJWu/dAfzAzB4zs6Pj7sxlHHT30/2fXwBwcJyduYxwGe9Res0y4xNz7gZZ/rwqfUD3h97n7n8K4EMAPtl/uzqRfPN/sEnKnW5rGe9Rucwy4783znM36PLnVY0j2E8BOLzl90P92yaCu5/qfz8L4HuYvKWoz7yygm7/+9kx9+f3JmkZ78stM44JOHfjXP58HMH+MwA3mNl1ZtYE8FEAD4yhH3/AzGb6H5zAzGYAfBCTtxT1AwDu7P98J4Dvj7EvrzIpy3inlhnHmM/d2Jc/d/eRfwG4FZufyP8KwD+Oow+Jfr0FwC/6XyfG3TcA38Tm27oONj/b+DiAKwE8DOAZAD8EsHeC+vZvAJ4E8AQ2A2t+TH17Hzbfoj8B4PH+163jPnekXyM5bxouK5IJfUAnkgkFu0gmFOwimVCwi2RCwS6SCQW7SCYU7CKZ+H8I7xIGwGcNegAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGO1q-UKQTMa",
        "outputId": "162643d9-41c8-4ac3-bdf2-459ad53b148d"
      },
      "source": [
        "!nvidia-smi -L\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla K80 (UUID: GPU-11f2e784-507f-748a-6855-e8d6b3bd7cab)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgqmLVufRjKy"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}